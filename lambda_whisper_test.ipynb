{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLAmZLtQPEpQ",
        "outputId": "9dabd2d6-640c-4321-bbbc-5387a1717e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 14 19:18:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4 -q\n",
        "!apt update -q\n",
        "!apt install -y ffmpeg -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Mu5pBePaM_",
        "outputId": "d357b68a-e93d-42fd-9168-fe6987e4966d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: add-apt-repository <sourceline>\n",
            "\n",
            "add-apt-repository is a script for adding apt sources.list entries.\n",
            "It can be used to add any repository and also provides a shorthand\n",
            "syntax for adding a Launchpad PPA (Personal Package Archive)\n",
            "repository.\n",
            "\n",
            "<sourceline> - The apt repository source line to add. This is one of:\n",
            "  a complete apt line in quotes,\n",
            "  a repo url and areas in quotes (areas defaults to 'main')\n",
            "  a PPA shortcut.\n",
            "  a distro component\n",
            "\n",
            "  Examples:\n",
            "    apt-add-repository 'deb http://myserver/path/to/repo stable myrepo'\n",
            "    apt-add-repository 'http://myserver/path/to/repo myrepo'\n",
            "    apt-add-repository 'https://packages.medibuntu.org free non-free'\n",
            "    apt-add-repository http://extras.ubuntu.com/ubuntu\n",
            "    apt-add-repository ppa:user/repository\n",
            "    apt-add-repository ppa:user/distro/repository\n",
            "    apt-add-repository multiverse\n",
            "\n",
            "If --remove is given the tool will remove the given sourceline from your\n",
            "sources.list\n",
            "\n",
            "\n",
            "add-apt-repository: error: no such option: -q\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Fetched 261 kB in 2s (130 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "22 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets>=2.6.1 -q\n",
        "!pip install git+https://github.com/huggingface/transformers -q\n",
        "!pip install librosa -q\n",
        "!pip install evaluate>=0.30 -q\n",
        "!pip install jiwer -q\n",
        "!pip install gradio -q\n",
        "!pip install more-itertools -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtb3JaCbPq-v",
        "outputId": "272cfc1d-7355-4c15-cb5c-131f98c2665d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import interleave_datasets, load_dataset\n",
        "\n",
        "def load_streaming_dataset(dataset_name, dataset_config_name, split, **kwargs):\n",
        "    if \"+\" in split:\n",
        "        # load multiple splits separated by the `+` symbol *with* streaming mode\n",
        "        dataset_splits = [load_dataset(dataset_name, dataset_config_name, split=split_name, streaming=True, **kwargs) for split_name in split.split(\"+\")]\n",
        "        # interleave multiple splits to form one dataset\n",
        "        interleaved_dataset = interleave_datasets(dataset_splits)\n",
        "        return interleaved_dataset\n",
        "    else:\n",
        "        # load a single split *with* streaming mode\n",
        "        dataset = load_dataset(dataset_name, dataset_config_name, split=split, streaming=True, **kwargs)\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "Y19yduPBQWdW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import IterableDatasetDict\n",
        "\n",
        "raw_datasets = IterableDatasetDict()\n",
        "\n",
        "raw_datasets[\"train\"] = load_streaming_dataset(\"mozilla-foundation/common_voice_11_0\", \"bn\", split=\"train\", use_auth_token=False).take(100)\n",
        "raw_datasets[\"test\"] = load_streaming_dataset(\"mozilla-foundation/common_voice_11_0\", \"bn\", split=\"test\", use_auth_token=False).take(2)"
      ],
      "metadata": {
        "id": "OOBkzXdKQZ_1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKcC8SOX4OEB",
        "outputId": "c0a73ddc-3adc-4e64-fe03-9b7a72a6f9af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<datasets.iterable_dataset.IterableDataset at 0x7f42c0464bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Bengali\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "AmDLLng7Qtg4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"].features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7ctHKKFQzd7",
        "outputId": "ed843676-5c82-4cc4-fe0a-2a82330d1fcd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'client_id': Value(dtype='string', id=None),\n",
              " 'path': Value(dtype='string', id=None),\n",
              " 'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
              " 'sentence': Value(dtype='string', id=None),\n",
              " 'up_votes': Value(dtype='int64', id=None),\n",
              " 'down_votes': Value(dtype='int64', id=None),\n",
              " 'age': Value(dtype='string', id=None),\n",
              " 'gender': Value(dtype='string', id=None),\n",
              " 'accent': Value(dtype='string', id=None),\n",
              " 'locale': Value(dtype='string', id=None),\n",
              " 'segment': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "raw_datasets = raw_datasets.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "TDmebH47Q68R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "do_lower_case = False\n",
        "do_remove_punctuation = False\n",
        "\n",
        "punctuation_to_remove = string.punctuation.replace(\"'\", \"\")  # don't remove apostrophes\n",
        "punctuation_to_remove_regex = f\"[{''.join(punctuation_to_remove)}]\"\n",
        "\n",
        "if do_remove_punctuation:\n",
        "    print(\"Removing punctuation: \", punctuation_to_remove)"
      ],
      "metadata": {
        "id": "VxW9i6gjQ-m5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and (possibly) resample audio datato 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array \n",
        "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    # compute input length of audio sample in seconds\n",
        "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
        "    \n",
        "    # optional pre-processing steps\n",
        "    transcription = batch[\"sentence\"]\n",
        "    if do_lower_case:\n",
        "        transcription = transcription.lower()\n",
        "    if do_remove_punctuation:\n",
        "        transcription = re.sub(punctuation_to_remove_regex, \" \", transcription).strip()\n",
        "    \n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = processor.tokenizer(transcription).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "-9ZEarrFRB6o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_datasets = raw_datasets.map(prepare_dataset, remove_columns=list(next(iter(raw_datasets.values())).features)).with_format(\"torch\")"
      ],
      "metadata": {
        "id": "vLJCIGh7RGbt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(\n",
        "    buffer_size=500,\n",
        "    seed=0,\n",
        ")"
      ],
      "metadata": {
        "id": "WZ8YrflNRJFS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 30.0\n",
        "\n",
        "def is_audio_in_length_range(length):\n",
        "    return length < max_input_length"
      ],
      "metadata": {
        "id": "ford3lITRLt3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].filter(\n",
        "    is_audio_in_length_range,\n",
        "    input_columns=[\"input_length\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Yegv-FH_ROTr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "ftLeLpH7RRir"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "OhYYPChhRUSc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "_oSjAZwoRX81"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate with the 'normalised' WER\n",
        "do_normalize_eval = True\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True, normalize=do_normalize_eval)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True, normalize=do_normalize_eval)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "sw5vXIW_Rao3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "2_39uX6ARf23"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "NXyRi1pcRjXm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-bn\",  # your repo name\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=10,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=32,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=30,\n",
        "    save_steps=10,\n",
        "    eval_steps=10,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "CDEuDGG7R3KD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "from transformers.trainer_pt_utils import IterableDatasetShard\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "# trainer callback to reinitialise and reshuffle the streamable datasets at the beginning of each epoch\n",
        "class ShuffleCallback(TrainerCallback):\n",
        "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
        "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
        "            pass  # set_epoch() is handled by the Trainer\n",
        "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
        "            train_dataloader.dataset.set_epoch(train_dataloader.dataset._epoch + 1)"
      ],
      "metadata": {
        "id": "Iayy5rCUR6J6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=vectorized_datasets[\"train\"],\n",
        "    eval_dataset=vectorized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor,\n",
        "    callbacks=[ShuffleCallback()],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgz6ppq_SNqt",
        "outputId": "b16edb6b-09c4-49c8-f2e7-f9cde2b10560"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using cuda_amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(training_args.output_dir)\n",
        "processor.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BMPluzySPr0",
        "outputId": "5bd7b42e-1fbc-4846-99d2-773d73ce436e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./whisper-small-bn/config.json\n",
            "Model weights saved in ./whisper-small-bn/pytorch_model.bin\n",
            "Feature extractor saved in ./whisper-small-bn/preprocessor_config.json\n",
            "tokenizer config file saved in ./whisper-small-bn/tokenizer_config.json\n",
            "Special tokens file saved in ./whisper-small-bn/special_tokens_map.json\n",
            "added tokens file saved in ./whisper-small-bn/added_tokens.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "rCYsWiGGSStC",
        "outputId": "901290a9-c5c4-49d5-bff2-997e8d60aaf7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 80\n",
            "  Num Epochs = 9223372036854775807\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10\n",
            "  Number of trainable parameters = 241734912\n",
            "Reading metadata...: 16777it [00:00, 25058.44it/s]\n",
            "The following columns in the training set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:42, Epoch 1/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.180656</td>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples: Unknown\n",
            "  Batch size = 32\n",
            "Reading metadata...: 8353it [00:00, 22321.10it/s]\n",
            "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "Saving model checkpoint to ./whisper-small-bn/checkpoint-10\n",
            "Configuration saved in ./whisper-small-bn/checkpoint-10/config.json\n",
            "Model weights saved in ./whisper-small-bn/checkpoint-10/pytorch_model.bin\n",
            "Feature extractor saved in ./whisper-small-bn/checkpoint-10/preprocessor_config.json\n",
            "tokenizer config file saved in ./whisper-small-bn/checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./whisper-small-bn/checkpoint-10/special_tokens_map.json\n",
            "added tokens file saved in ./whisper-small-bn/checkpoint-10/added_tokens.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./whisper-small-bn/checkpoint-10 (score: 100.0).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=2.2869487762451173, metrics={'train_runtime': 59.8975, 'train_samples_per_second': 1.336, 'train_steps_per_second': 0.167, 'total_flos': 2.30868320256e+16, 'train_loss': 2.2869487762451173, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(test_dataset=vectorized_datasets[\"test\"])"
      ],
      "metadata": {
        "id": "4GJO5zFbjLJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e7c1f4-47ce-4898-daef-ab05b75b39fe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples: Unknown\n",
            "  Batch size = 32\n",
            "Reading metadata...: 8353it [00:00, 21623.82it/s]\n",
            "The following columns in the test set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = processor.tokenizer.convert_tokens_to_string([x for x in processor.tokenizer.convert_ids_to_tokens(preds[0][0]) if not x.startswith('<')])"
      ],
      "metadata": {
        "id": "fdMXBKFEnzpq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.tokenizer.convert_tokens_to_string([x for x in processor.tokenizer.convert_ids_to_tokens(preds[0][1]) if not x.startswith('<')])"
      ],
      "metadata": {
        "id": "PuN9LXzwj7bP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f408f8d6-44f2-47a3-f140-e72b4c9f9913"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" H. Ronny's land a cricket dollar who you can't see me.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Data"
      ],
      "metadata": {
        "id": "11TNxhJu_gTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o = []\n",
        "for i in raw_datasets[\"test\"]:\n",
        "  o.append((i['sentence']))\n",
        "  break"
      ],
      "metadata": {
        "id": "b-0duEQ4mOBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9efdcc3c-092d-43d9-84b5-f0b00fdb092d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 8353it [00:00, 16835.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Original and Predicted data"
      ],
      "metadata": {
        "id": "Vgpwe7PGAInR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original : \", o[0])\n",
        "print(\"Predicted : \", p)"
      ],
      "metadata": {
        "id": "uws9onm7mWpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc886da-096a-4efc-e579-0376bfb4fd85"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original :  গভীর জলের বার্থ ও বহুমুখী টার্মিনাল সহ, বন্দরটি দক্ষতার সাথে বিশ্বের বৃহত্তম বাল্ক ক্যারিয়ার পরিচালনা করতে সক্ষম।\n",
            "Predicted :   Gavir Jaller Barth and Bohumukhi Terminal Shah are especially interested in the construction of the bulk carrier.\n"
          ]
        }
      ]
    }
  ]
}